{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Any Inference \ud83d\ude80 \ud83d\udea7 This project is in WIP any-inference is a python package that uses a message broker, rabbitmq , to send, consume and receive replies. It's aim to ease machine learning model inference since we can take advantage of the message queues to process and batch multiple messages. Moreover, when used with an HTTP server (e.g. flask ) we can use as many process as we want to handle the incoming requests. How it works The main idea is to use two queues, inputs and outputs . The Producer will place messages into a inputs queue (here shown with ids=1,2,3 ) and the Consumer will consume them. Assume the Consumer will use this messages as input to a machine learning model , e.g. the messages contains images and we want to predict classes. Once the Consumer is done, it will place it's reply messages back to the outputs queue from which the Producer can read. Batch Inference We can also go one step further, once the Consumer receive a message, it will wait a little bit and consume all the messages in the queue in one go. This will allow us to batch message together and send them for inference. Getting Started Head over the getting started section. Examples Head over the examples section.","title":"Docs"},{"location":"#any-inference","text":"\ud83d\udea7 This project is in WIP any-inference is a python package that uses a message broker, rabbitmq , to send, consume and receive replies. It's aim to ease machine learning model inference since we can take advantage of the message queues to process and batch multiple messages. Moreover, when used with an HTTP server (e.g. flask ) we can use as many process as we want to handle the incoming requests.","title":"Any Inference \ud83d\ude80"},{"location":"#how-it-works","text":"The main idea is to use two queues, inputs and outputs . The Producer will place messages into a inputs queue (here shown with ids=1,2,3 ) and the Consumer will consume them. Assume the Consumer will use this messages as input to a machine learning model , e.g. the messages contains images and we want to predict classes. Once the Consumer is done, it will place it's reply messages back to the outputs queue from which the Producer can read.","title":"How it works"},{"location":"#batch-inference","text":"We can also go one step further, once the Consumer receive a message, it will wait a little bit and consume all the messages in the queue in one go. This will allow us to batch message together and send them for inference.","title":"Batch Inference"},{"location":"#getting-started","text":"Head over the getting started section.","title":"Getting Started"},{"location":"#examples","text":"Head over the examples section.","title":"Examples"},{"location":"getting_started/","text":"Getting Started Installation You can install the package using pip pip install git+https://github.com/FrancescoSaverioZuppichini/any-inference.git Rabbitmq You will also need rabbitmq you can run locally it using docker docker run --rm -it -p 15672:15672 -p 5672:5672 rabbitmq:management Simple Example Warning So far only the mode advance WaitAndPopConsumer consumer is implement. Once it receives a message, it will wait a little bit, batch and send batch the message's replies. Let's create one producer and one consumer # producer.py from any_inference import Producer import time producer = Producer () def send (): data = { \"foo\" : \"baa\" , } try : res = producer . send ( data ) print ( res ) except TimeoutError : print ( \"Timeout\" ) for i in range ( 36 ): send () time . sleep ( 0.05 ) # consumer.py from any_inference import WaitAndPopConsumer from typing import List , Dict , Any def inference ( messages : List [ Dict [ str , Any ]]): # here you can have your thicc model print ( f \"Seen { ',' . join ([ m [ 'uid' ] for m in messages ]) } \" ) return messages WaitAndPopConsumer ( inference_strategy = inference ) . spin () Now, it two different terminal, let's start the consumer first and the publisher later $ python consumer.py [12:48:14] INFO \u2705 Connected to Consumer.py:52 amqp://guest:guest@localhost// And the consumer $ python producer.py [12:48:14] INFO \u2705 Connected to Consumer.py:52 amqp://guest:guest@localhost// As soon as we start the publisher we will see the messages replies, in our case we are just sending back the messages we received $ python producer.py [12:48:14] INFO \u2705 Connected to Consumer.py:52 amqp://guest:guest@localhost// Sending data={'foo': 'baa'} INFO [60812] received e0b7db7d-ee2b-11ec- Producer.py:97 95b2-309c23a77245, waiting for e0b7db7d-ee2b-11ec-95b2-309c23a77245 INFO [60812] ack Producer.py:99 e0b7db7d-ee2b-11ec-95b2-309c23a77245 Received {'foo': 'baa', 'pid': '60812', 'uid': 'e0b7db7d-ee2b-11ec-95b2-309c23a77245'}) Sending data={'foo': 'baa'} INFO [60812] received e0e489a1-ee2b-11ec- Producer.py:97 961f-309c23a77245, waiting for e0e489a1-ee2b-11ec-961f-309c23a77245 INFO [60812] ack Producer.py:99 e0e489a1-ee2b-11ec-961f-309c23a77245 Received {'foo': 'baa', 'pid': '60812', 'uid': 'e0e489a1-ee2b-11ec-961f-309c23a77245'}) While, from the consumer side we will see all the loggings $ python consumer.py [12:48:14] INFO \u2705 Connected to Consumer.py:52 amqp://guest:guest@localhost// [12:54:34] INFO \u2705 Connected to Consumer.py:52 amqp://guest:guest@localhost// [12:54:36] INFO [60773] \ud83d\udd14 60812->e0b7db7d-ee2b-11e Consumer.py:177 c-95b2-309c23a77245 INFO [ThreadPoolExecutor-0_0] \ud83c\udfc3 ... Consumer.py:144 INFO [ThreadPoolExecutor-0_0] \ud83d\udce6 Consumer.py:150 Batching 1. INFO [ThreadPoolExecutor-0_0] \ud83d\udce6 message Consumer.py:159 s=e0b7db7d-ee2b-11ec-95b2-309c23a77 245 Seen e0b7db7d-ee2b-11ec-95b2-309c23a77245 INFO [ThreadPoolExecutor-0_0] Done! Consumer.py:174 INFO [ThreadPoolExecutor-1_0] e0b7db7d-e Consumer.py:139 e2b-11ec-95b2-309c23a77245 -> 60812 INFO [60773] \ud83d\udd14 60812->e0e489a1-ee2b-11e Consumer.py:177 c-961f-309c23a77245 INFO [ThreadPoolExecutor-0_0] \ud83c\udfc3 ... Consumer.py:144 INFO [ThreadPoolExecutor-0_0] \ud83d\udce6 Consumer.py:150 Batching 1. INFO [ThreadPoolExecutor-0_0] \ud83d\udce6 message Consumer.py:159 s=e0e489a1-ee2b-11ec-961f-309c23a77 245 Seen e0e489a1-ee2b-11ec-961f-309c23a77245 INFO [ThreadPoolExecutor-0_0] Done! Consumer.py:174 INFO [ThreadPoolExecutor-1_0] e0e489a1-e Consumer.py:139 e2b-11ec-961f-309c23a77245 -> 60812 INFO [60773] \ud83d\udd14 60812->e10e5a76-ee2b-11e Consumer.py:177 c-828e-309c23a77245 INFO [ThreadPoolExecutor-0_0] \ud83c\udfc3 ... Consumer.py:144 [12:54:37] INFO [ThreadPoolExecutor-0_0] \ud83d\udce6 Consumer.py:150 Batching 1. INFO [ThreadPoolExecutor-0_0] \ud83d\udce6 message Consumer.py:159 s=e10e5a76-ee2b-11ec-828e-309c23a77 245 Seen e10e5a76-ee2b-11ec-828e-309c23a77245 INFO [ThreadPoolExecutor-0_0] Done! Consumer.py:174 INFO [ThreadPoolExecutor-1_0] e10e5a76-e Consumer.py:139 e2b-11ec-828e-309c23a77245 -> 60812 INFO [60773] \ud83d\udd14 60812->e13825bd-ee2b-11e Consumer.py:177 c-a590-309c23a77245 INFO [ThreadPoolExecutor-0_0] \ud83c\udfc3 ... Consumer.py:144 INFO [ThreadPoolExecutor-0_0] \ud83d\udce6 Consumer.py:150 Batching 1. INFO [ThreadPoolExecutor-0_0] \ud83d\udce6 message Consumer.py:159 s=e13825bd-ee2b-11ec-a590-309c23a77 245 Seen e13825bd-ee2b-11ec-a590-309c23a77245 INFO [ThreadPoolExecutor-0_0] Done! Consumer.py:174 INFO [ThreadPoolExecutor-1_0] e13825bd-e Consumer.py:139 e2b-11ec-a590-309c23a77245 -> 60812","title":"Getting Started"},{"location":"getting_started/#getting-started","text":"","title":"Getting Started"},{"location":"getting_started/#installation","text":"You can install the package using pip pip install git+https://github.com/FrancescoSaverioZuppichini/any-inference.git","title":"Installation"},{"location":"getting_started/#rabbitmq","text":"You will also need rabbitmq you can run locally it using docker docker run --rm -it -p 15672:15672 -p 5672:5672 rabbitmq:management","title":"Rabbitmq"},{"location":"getting_started/#simple-example","text":"Warning So far only the mode advance WaitAndPopConsumer consumer is implement. Once it receives a message, it will wait a little bit, batch and send batch the message's replies. Let's create one producer and one consumer # producer.py from any_inference import Producer import time producer = Producer () def send (): data = { \"foo\" : \"baa\" , } try : res = producer . send ( data ) print ( res ) except TimeoutError : print ( \"Timeout\" ) for i in range ( 36 ): send () time . sleep ( 0.05 ) # consumer.py from any_inference import WaitAndPopConsumer from typing import List , Dict , Any def inference ( messages : List [ Dict [ str , Any ]]): # here you can have your thicc model print ( f \"Seen { ',' . join ([ m [ 'uid' ] for m in messages ]) } \" ) return messages WaitAndPopConsumer ( inference_strategy = inference ) . spin () Now, it two different terminal, let's start the consumer first and the publisher later $ python consumer.py [12:48:14] INFO \u2705 Connected to Consumer.py:52 amqp://guest:guest@localhost// And the consumer $ python producer.py [12:48:14] INFO \u2705 Connected to Consumer.py:52 amqp://guest:guest@localhost// As soon as we start the publisher we will see the messages replies, in our case we are just sending back the messages we received $ python producer.py [12:48:14] INFO \u2705 Connected to Consumer.py:52 amqp://guest:guest@localhost// Sending data={'foo': 'baa'} INFO [60812] received e0b7db7d-ee2b-11ec- Producer.py:97 95b2-309c23a77245, waiting for e0b7db7d-ee2b-11ec-95b2-309c23a77245 INFO [60812] ack Producer.py:99 e0b7db7d-ee2b-11ec-95b2-309c23a77245 Received {'foo': 'baa', 'pid': '60812', 'uid': 'e0b7db7d-ee2b-11ec-95b2-309c23a77245'}) Sending data={'foo': 'baa'} INFO [60812] received e0e489a1-ee2b-11ec- Producer.py:97 961f-309c23a77245, waiting for e0e489a1-ee2b-11ec-961f-309c23a77245 INFO [60812] ack Producer.py:99 e0e489a1-ee2b-11ec-961f-309c23a77245 Received {'foo': 'baa', 'pid': '60812', 'uid': 'e0e489a1-ee2b-11ec-961f-309c23a77245'}) While, from the consumer side we will see all the loggings $ python consumer.py [12:48:14] INFO \u2705 Connected to Consumer.py:52 amqp://guest:guest@localhost// [12:54:34] INFO \u2705 Connected to Consumer.py:52 amqp://guest:guest@localhost// [12:54:36] INFO [60773] \ud83d\udd14 60812->e0b7db7d-ee2b-11e Consumer.py:177 c-95b2-309c23a77245 INFO [ThreadPoolExecutor-0_0] \ud83c\udfc3 ... Consumer.py:144 INFO [ThreadPoolExecutor-0_0] \ud83d\udce6 Consumer.py:150 Batching 1. INFO [ThreadPoolExecutor-0_0] \ud83d\udce6 message Consumer.py:159 s=e0b7db7d-ee2b-11ec-95b2-309c23a77 245 Seen e0b7db7d-ee2b-11ec-95b2-309c23a77245 INFO [ThreadPoolExecutor-0_0] Done! Consumer.py:174 INFO [ThreadPoolExecutor-1_0] e0b7db7d-e Consumer.py:139 e2b-11ec-95b2-309c23a77245 -> 60812 INFO [60773] \ud83d\udd14 60812->e0e489a1-ee2b-11e Consumer.py:177 c-961f-309c23a77245 INFO [ThreadPoolExecutor-0_0] \ud83c\udfc3 ... Consumer.py:144 INFO [ThreadPoolExecutor-0_0] \ud83d\udce6 Consumer.py:150 Batching 1. INFO [ThreadPoolExecutor-0_0] \ud83d\udce6 message Consumer.py:159 s=e0e489a1-ee2b-11ec-961f-309c23a77 245 Seen e0e489a1-ee2b-11ec-961f-309c23a77245 INFO [ThreadPoolExecutor-0_0] Done! Consumer.py:174 INFO [ThreadPoolExecutor-1_0] e0e489a1-e Consumer.py:139 e2b-11ec-961f-309c23a77245 -> 60812 INFO [60773] \ud83d\udd14 60812->e10e5a76-ee2b-11e Consumer.py:177 c-828e-309c23a77245 INFO [ThreadPoolExecutor-0_0] \ud83c\udfc3 ... Consumer.py:144 [12:54:37] INFO [ThreadPoolExecutor-0_0] \ud83d\udce6 Consumer.py:150 Batching 1. INFO [ThreadPoolExecutor-0_0] \ud83d\udce6 message Consumer.py:159 s=e10e5a76-ee2b-11ec-828e-309c23a77 245 Seen e10e5a76-ee2b-11ec-828e-309c23a77245 INFO [ThreadPoolExecutor-0_0] Done! Consumer.py:174 INFO [ThreadPoolExecutor-1_0] e10e5a76-e Consumer.py:139 e2b-11ec-828e-309c23a77245 -> 60812 INFO [60773] \ud83d\udd14 60812->e13825bd-ee2b-11e Consumer.py:177 c-a590-309c23a77245 INFO [ThreadPoolExecutor-0_0] \ud83c\udfc3 ... Consumer.py:144 INFO [ThreadPoolExecutor-0_0] \ud83d\udce6 Consumer.py:150 Batching 1. INFO [ThreadPoolExecutor-0_0] \ud83d\udce6 message Consumer.py:159 s=e13825bd-ee2b-11ec-a590-309c23a77 245 Seen e13825bd-ee2b-11ec-a590-309c23a77245 INFO [ThreadPoolExecutor-0_0] Done! Consumer.py:174 INFO [ThreadPoolExecutor-1_0] e13825bd-e Consumer.py:139 e2b-11ec-a590-309c23a77245 -> 60812","title":"Simple Example"},{"location":"reference/SUMMARY/","text":"","title":"SUMMARY"}]}